{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ee034a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.9.1+cu128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# Import from our sudoku.py module\n",
    "sys.path.append('.')\n",
    "from sudoku import (\n",
    "    SudokuDataset, SudokuMDM, MDMTrainer, MDMSampler,\n",
    "    MaskSchedule, check_sudoku_valid, evaluate_samples\n",
    ")\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Device configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4087b5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yifan/neural_mi/.conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model_path = Path('./checkpoints/sudoku_mdm_best.pt')\n",
    "model_config = {\n",
    "    'vocab_size': 10,  # 0 (mask) + 1-9 (digits)\n",
    "    'd_model': 384,\n",
    "    'nhead': 12,\n",
    "    'num_layers': 4,\n",
    "    'dim_feedforward': 512,\n",
    "    'dropout': 0.1,\n",
    "    'max_seq_len': 81  # 9x9 Sudoku\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = SudokuMDM(**model_config).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9418200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conditional_entropy_data(\n",
    "    model: SudokuMDM, puzzles: torch.Tensor, k=9, batch_size=256\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate ground truth conditional entropy matrices for multiple puzzles (batched).\n",
    "    \n",
    "    For each position i, we compute H(j | i, c) = sum_xi P(xi | c) * H(j | xi, c)\n",
    "    \n",
    "    Args:\n",
    "        model: Trained SudokuMDM\n",
    "        puzzles: [num_puzzles, 81] tensor of input puzzles\n",
    "        k: Number of top tokens to consider (max 9 for digits 1-9)\n",
    "        batch_size: Number of (puzzle, position, token) tuples to process at once\n",
    "    \n",
    "    Returns:\n",
    "        CE_matrices: [num_puzzles, 81, 81] tensor of conditional entropy values H(j | i)\n",
    "    \"\"\"\n",
    "    # Ensure puzzles is on the correct device and has right shape\n",
    "    if puzzles.dim() == 1:\n",
    "        puzzles = puzzles.unsqueeze(0)\n",
    "    \n",
    "    puzzles = puzzles.to(device)\n",
    "    num_puzzles = puzzles.size(0)\n",
    "\n",
    "    # print(f\"Generating conditional entropy data for {num_puzzles} puzzles (batched)...\")\n",
    "    # print(f\"Using top-{k} tokens weighted by their probabilities\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Initial forward pass to get P(j | c) for all puzzles\n",
    "        initial_logits = model(puzzles)  # Shape: (num_puzzles, 81, 10)\n",
    "        initial_probs = F.softmax(initial_logits, dim=-1)  # Shape: (num_puzzles, 81, 10)\n",
    "        \n",
    "        CE_matrices = torch.zeros((num_puzzles, 81, 81)).to(device)\n",
    "        \n",
    "        # Build list of all (puzzle_idx, position, token, weight) tuples to process\n",
    "        all_queries = []  # List of (puzzle_idx, pos_i, token, weight)\n",
    "        \n",
    "        for p_idx in range(num_puzzles):\n",
    "            # Find masked positions for this puzzle\n",
    "            masked_positions = (puzzles[p_idx] == 0).nonzero(as_tuple=True)[0].tolist()\n",
    "            \n",
    "            for i in masked_positions:\n",
    "                # Get top-k tokens and their probabilities for position i\n",
    "                topk_probs, topk_tokens = torch.topk(initial_probs[p_idx, i, 1:], k)\n",
    "                topk_tokens += 1  # Adjust indices since we excluded token 0\n",
    "                \n",
    "                # Normalize probabilities\n",
    "                weights = topk_probs / topk_probs.sum()\n",
    "                \n",
    "                for idx, token in enumerate(topk_tokens):\n",
    "                    all_queries.append((p_idx, i, token.item(), weights[idx].item()))\n",
    "        \n",
    "        # print(f\"Total queries to process: {len(all_queries)}\")\n",
    "        \n",
    "        # Process in batches\n",
    "        num_batches = (len(all_queries) + batch_size - 1) // batch_size\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            start = batch_idx * batch_size\n",
    "            end = min(start + batch_size, len(all_queries))\n",
    "            batch_queries = all_queries[start:end]\n",
    "            \n",
    "            # Create batched input - each query modifies its corresponding puzzle\n",
    "            batch_puzzles = torch.zeros((len(batch_queries), 81), dtype=torch.long, device=device)\n",
    "            \n",
    "            for b, (p_idx, pos_i, token, weight) in enumerate(batch_queries):\n",
    "                batch_puzzles[b] = puzzles[p_idx].clone()\n",
    "                batch_puzzles[b, pos_i] = token\n",
    "            \n",
    "            # Batched forward pass\n",
    "            logits = model(batch_puzzles)  # Shape: (B, 81, 10)\n",
    "            probs = F.softmax(logits, dim=-1)  # Shape: (B, 81, 10)\n",
    "            \n",
    "            # H(j | x_i) for all j, for each sample in batch\n",
    "            H_j_given_xi = -torch.sum(probs * torch.log(probs + 1e-10), dim=-1)  # Shape: (B, 81)\n",
    "            \n",
    "            # Accumulate weighted contributions to the correct puzzle's CE matrix\n",
    "            for b, (p_idx, pos_i, token, weight) in enumerate(batch_queries):\n",
    "                CE_matrices[p_idx, pos_i] += weight * H_j_given_xi[b]\n",
    "            \n",
    "            if (batch_idx + 1) % 20 == 0 or batch_idx == num_batches - 1:\n",
    "                #print(f\"  Processed batch {batch_idx + 1}/{num_batches}\")\n",
    "                ...\n",
    "        # CE should be non-negative\n",
    "        CE_matrices = torch.clamp(CE_matrices, min=0)\n",
    "        \n",
    "    return CE_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6913e7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE Predictor:\n",
      "  Trainable parameters: 37,185\n",
      "  Total parameters (including frozen MDM): 4,195,531\n",
      "  MDM parameters (frozen): 4,158,346\n"
     ]
    }
   ],
   "source": [
    "class CEPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight module to predict conditional entropy matrix H(j | i, c) given a puzzle c.\n",
    "    \n",
    "    Uses frozen embeddings from a pre-trained MDM model, then adds a small trainable head\n",
    "    to predict pairwise conditional entropy.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Get contextualized embeddings from frozen MDM (no grad)\n",
    "    2. Project to smaller dimension\n",
    "    3. Bilinear pairwise prediction head\n",
    "    \n",
    "    Output: [batch_size, 81, 81] matrix where entry (i, j) = H(X_j | X_i, c)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        mdm_model: SudokuMDM,\n",
    "        hidden_dim: int = 64,\n",
    "        dropout: float = 0.1,\n",
    "        freeze_mdm: bool = True\n",
    "    ):\n",
    "        super(CEPredictor, self).__init__()\n",
    "        \n",
    "        self.mdm = mdm_model\n",
    "        self.mdm_dim = mdm_model.d_model  # e.g., 384\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Freeze MDM parameters\n",
    "        if freeze_mdm:\n",
    "            for param in self.mdm.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Project MDM embeddings to smaller dimension\n",
    "        self.proj = nn.Linear(self.mdm_dim, hidden_dim)\n",
    "        \n",
    "        # Separate projections for \"source\" (i) and \"target\" (j) roles\n",
    "        self.query_proj = nn.Linear(hidden_dim, hidden_dim)  # \"if I reveal i...\"\n",
    "        self.key_proj = nn.Linear(hidden_dim, hidden_dim)    # \"...entropy of j\"\n",
    "        \n",
    "        # Small MLP head for final prediction\n",
    "        self.ce_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        # Only init the trainable layers (not MDM)\n",
    "        for module in [self.proj, self.query_proj, self.key_proj, self.ce_head]:\n",
    "            for p in module.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def get_mdm_embeddings(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract contextualized embeddings from the MDM.\n",
    "        \n",
    "        Args:\n",
    "            x: [batch_size, 81] - puzzle tokens\n",
    "            \n",
    "        Returns:\n",
    "            h: [batch_size, 81, mdm_dim] - contextualized representations\n",
    "        \"\"\"\n",
    "        # Run through MDM's embedding + positional encoding + transformer\n",
    "        # but stop before the output head\n",
    "        with torch.no_grad():\n",
    "            h = self.mdm.embedding(x)  # [B, 81, d_model]\n",
    "            h = self.mdm.pos_encoder(h)  # Add positional encoding\n",
    "            h = self.mdm.transformer(h)  # [B, 81, d_model]\n",
    "        return h\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, 81] - puzzle with token indices (0 = mask, 1-9 = digits)\n",
    "            \n",
    "        Returns:\n",
    "            ce_matrix: [batch_size, 81, 81] - predicted H(j | i, c) for all pairs\n",
    "        \"\"\"\n",
    "        # Get frozen MDM embeddings\n",
    "        h = self.get_mdm_embeddings(x)  # [B, 81, mdm_dim]\n",
    "        h.requires_grad = True\n",
    "        # Project to smaller dimension\n",
    "        h = self.proj(h)  # [B, 81, hidden_dim]\n",
    "        \n",
    "        # Compute pairwise CE predictions\n",
    "        # Query: \"if I reveal position i...\"\n",
    "        # Key: \"...what is the entropy of position j?\"\n",
    "        queries = self.query_proj(h)  # [B, 81, hidden_dim]\n",
    "        keys = self.key_proj(h)       # [B, 81, hidden_dim]\n",
    "        \n",
    "        # Pairwise interaction: element-wise product\n",
    "        # [B, 81, 1, hidden_dim] * [B, 1, 81, hidden_dim] -> [B, 81, 81, hidden_dim]\n",
    "        interaction = queries.unsqueeze(2) * keys.unsqueeze(1)\n",
    "        \n",
    "        # Predict CE from interaction features\n",
    "        ce_matrix = self.ce_head(interaction).squeeze(-1)  # [B, 81, 81]\n",
    "        \n",
    "        # CE must be non-negative\n",
    "        ce_matrix = F.softplus(ce_matrix)\n",
    "        \n",
    "        return ce_matrix\n",
    "\n",
    "\n",
    "class CEPredictorTrainer:\n",
    "    \"\"\"\n",
    "    Trainer for the CE predictor with on-the-fly CE generation.\n",
    "    \n",
    "    Instead of using pre-computed CE matrices, this trainer:\n",
    "    1. Takes solutions from a DataLoader\n",
    "    2. Applies random masking to create puzzles\n",
    "    3. Generates ground truth CE matrices on-the-fly\n",
    "    4. Trains the predictor on these\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        predictor: CEPredictor,\n",
    "        dataset: SudokuDataset,\n",
    "        mask_schedule: MaskSchedule,\n",
    "        lr: float = 1e-3,\n",
    "        weight_decay: float = 0.01,\n",
    "        batch_size: int = 32,\n",
    "        ce_batch_size: int = 256,\n",
    "        k: int = 9\n",
    "    ):\n",
    "        self.predictor = predictor\n",
    "        self.dataset = dataset\n",
    "        self.mask_schedule = mask_schedule\n",
    "        self.batch_size = batch_size\n",
    "        self.ce_batch_size = ce_batch_size  # batch size for CE generation\n",
    "        self.k = k  # top-k tokens for CE computation\n",
    "        \n",
    "        # Create DataLoader\n",
    "        self.dataloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "        self.dataloader_iter = iter(self.dataloader)\n",
    "        \n",
    "        # Only optimize the trainable parameters (not frozen MDM)\n",
    "        trainable_params = [p for p in predictor.parameters() if p.requires_grad]\n",
    "        print(f\"Trainable parameters: {sum(p.numel() for p in trainable_params):,}\")\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            trainable_params,\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer, T_max=1000\n",
    "        )\n",
    "        self.train_losses = []\n",
    "    \n",
    "    def _get_batch(self):\n",
    "        \"\"\"Get next batch from dataloader, reset if exhausted.\"\"\"\n",
    "        try:\n",
    "            batch = next(self.dataloader_iter)\n",
    "        except StopIteration:\n",
    "            self.dataloader_iter = iter(self.dataloader)\n",
    "            batch = next(self.dataloader_iter)\n",
    "        return batch\n",
    "    \n",
    "    def _apply_random_masking(self, solutions: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply random masking to solutions to create puzzles.\n",
    "        \n",
    "        Args:\n",
    "            solutions: [batch_size, 81] - complete Sudoku solutions\n",
    "            \n",
    "        Returns:\n",
    "            puzzles: [batch_size, 81] - masked puzzles\n",
    "        \"\"\"\n",
    "        puzzles = solutions.clone()\n",
    "        t = np.random.randint(1, self.mask_schedule.total_steps + 1)  # At least 1 mask\n",
    "        mask_ratio = self.mask_schedule.get_mask_ratio(t)\n",
    "        puzzles, _ = self.mask_schedule.apply_mask(puzzles, mask_ratio)\n",
    "        return puzzles\n",
    "    \n",
    "    def train_step(self) -> float:\n",
    "        \"\"\"\n",
    "        Single training step with on-the-fly CE generation.\n",
    "        \n",
    "        Returns:\n",
    "            loss value\n",
    "        \"\"\"\n",
    "        self.predictor.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Get batch of solutions\n",
    "        _, solutions = self._get_batch()\n",
    "        solutions = solutions.to(next(self.predictor.parameters()).device)\n",
    "        \n",
    "        # Apply random masking to create puzzles\n",
    "        puzzles = self._apply_random_masking(solutions)\n",
    "        \n",
    "        # Generate ground truth CE matrices on-the-fly\n",
    "        target_ce = generate_conditional_entropy_data(\n",
    "            self.predictor.mdm,\n",
    "            puzzles,\n",
    "            k=self.k,\n",
    "            batch_size=self.ce_batch_size\n",
    "        )\n",
    "        \n",
    "        # Forward pass\n",
    "        pred_ce = self.predictor(puzzles)  # [B, 81, 81]\n",
    "        \n",
    "        # Only compute loss on masked positions (rows where puzzle[i] == 0)\n",
    "        mask = (puzzles == 0)  # [B, 81]\n",
    "        row_mask = mask.unsqueeze(2).expand_as(pred_ce)  # [B, 81, 81]\n",
    "        \n",
    "        # MSE loss on valid entries\n",
    "        if row_mask.any():\n",
    "            loss = F.mse_loss(pred_ce[row_mask], target_ce[row_mask])\n",
    "            loss.backward()\n",
    "        else:\n",
    "            loss = torch.tensor(0.0, device=puzzles.device)\n",
    "        \n",
    "        pred_ce: torch.Tensor\n",
    "\n",
    "        # Backward pass\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(self.predictor.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        self.train_losses.append(loss.item())\n",
    "        return loss.item()\n",
    "    \n",
    "    def train_epoch(self, steps_per_epoch: int = None) -> float:\n",
    "        \"\"\"\n",
    "        Train for one epoch (or specified number of steps).\n",
    "        \n",
    "        Args:\n",
    "            steps_per_epoch: Number of steps. If None, use len(dataloader).\n",
    "            \n",
    "        Returns:\n",
    "            Average loss for the epoch\n",
    "        \"\"\"\n",
    "        if steps_per_epoch is None:\n",
    "            steps_per_epoch = len(self.dataloader)\n",
    "        \n",
    "        epoch_losses = []\n",
    "        for step in range(steps_per_epoch):\n",
    "            loss = self.train_step()\n",
    "            epoch_losses.append(loss)\n",
    "            \n",
    "            if (step + 1) % 10 == 0:\n",
    "                avg_loss = sum(epoch_losses[-10:]) / min(10, len(epoch_losses))\n",
    "                print(f\"  Step {step + 1}/{steps_per_epoch}, Loss: {avg_loss:.6f}\")\n",
    "        \n",
    "        return sum(epoch_losses) / len(epoch_losses)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, num_samples: int = 100) -> dict:\n",
    "        \"\"\"Evaluate predictor on random validation samples.\"\"\"\n",
    "        self.predictor.eval()\n",
    "        \n",
    "        # Generate some validation puzzles\n",
    "        \"\"\"TypeError: expected Tensor as element 0 in argument 0, but got tuple\"\"\"\n",
    "        solutions = torch.stack([self.dataset[i][1] for i in range(num_samples)])\n",
    "        solutions = solutions.to(next(self.predictor.parameters()).device)\n",
    "        puzzles = self._apply_random_masking(solutions)\n",
    "        \n",
    "        # Generate ground truth CE\n",
    "        target_ce = generate_conditional_entropy_data(\n",
    "            self.predictor.mdm,\n",
    "            puzzles,\n",
    "            k=self.k,\n",
    "            batch_size=self.ce_batch_size\n",
    "        )\n",
    "        \n",
    "        # Predict\n",
    "        pred_ce = self.predictor(puzzles)\n",
    "        \n",
    "        mask = (puzzles == 0)\n",
    "        row_mask = mask.unsqueeze(2).expand_as(pred_ce)\n",
    "        \n",
    "        if row_mask.any():\n",
    "            mse = F.mse_loss(pred_ce[row_mask], target_ce[row_mask]).item()\n",
    "            mae = F.l1_loss(pred_ce[row_mask], target_ce[row_mask]).item()\n",
    "        else:\n",
    "            mse, mae = 0.0, 0.0\n",
    "        \n",
    "        return {'mse': mse, 'mae': mae}\n",
    "\n",
    "\n",
    "# Create predictor that uses the frozen MDM embeddings\n",
    "ce_predictor = CEPredictor(\n",
    "    mdm_model=model,\n",
    "    hidden_dim=64,\n",
    "    dropout=0.1,\n",
    "    freeze_mdm=True\n",
    ").to(device)\n",
    "\n",
    "# Count only trainable parameters\n",
    "trainable_params = sum(p.numel() for p in ce_predictor.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in ce_predictor.parameters())\n",
    "print(f\"CE Predictor:\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Total parameters (including frozen MDM): {total_params:,}\")\n",
    "print(f\"  MDM parameters (frozen): {total_params - trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46fa6bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Sudoku from CSV: ./data/sudoku.csv (start=0, n=100000)\n",
      "✓ Loaded 100000 puzzles from CSV: ./data/sudoku.csv\n",
      "Trainable parameters: 37,185\n",
      "Epoch 1/10\n",
      "  Step 10/100, Loss: 9948705.338833\n",
      "  Step 20/100, Loss: 2.622402\n",
      "  Step 30/100, Loss: 3.006485\n",
      "  Step 40/100, Loss: 2.504423\n",
      "  Step 50/100, Loss: 2.730825\n",
      "  Step 60/100, Loss: 3.220881\n",
      "  Step 70/100, Loss: 2.807148\n",
      "  Step 80/100, Loss: 2.405799\n",
      "  Step 90/100, Loss: 2.623364\n",
      "  Step 100/100, Loss: 2.519560\n",
      "  Average Loss: 994872.977972\n",
      "  Eval MSE: 2.081733, MAE: 1.394552\n",
      "Epoch 2/10\n",
      "  Step 10/100, Loss: 2.638185\n",
      "  Step 20/100, Loss: 2.380998\n",
      "  Step 30/100, Loss: 2.245148\n",
      "  Step 40/100, Loss: 2.519163\n",
      "  Step 50/100, Loss: 2.418856\n",
      "  Step 60/100, Loss: 2.624129\n",
      "  Step 70/100, Loss: 2.690426\n",
      "  Step 80/100, Loss: 2.799759\n",
      "  Step 90/100, Loss: 2.730187\n",
      "  Step 100/100, Loss: 2.728065\n",
      "  Average Loss: 2.577492\n",
      "  Eval MSE: 1.671863, MAE: 1.138784\n",
      "Epoch 3/10\n",
      "  Step 10/100, Loss: 2.227292\n",
      "  Step 20/100, Loss: 2.932367\n",
      "  Step 30/100, Loss: 2.578526\n",
      "  Step 40/100, Loss: 2.149422\n",
      "  Step 50/100, Loss: 2.693855\n",
      "  Step 60/100, Loss: 2.732020\n",
      "  Step 70/100, Loss: 3.086859\n",
      "  Step 80/100, Loss: 2.722974\n",
      "  Step 90/100, Loss: 2.669477\n",
      "  Step 100/100, Loss: 2.716022\n",
      "  Average Loss: 2.650881\n",
      "  Eval MSE: 1.702842, MAE: 1.061662\n",
      "Epoch 4/10\n",
      "  Step 10/100, Loss: 2.508142\n",
      "  Step 20/100, Loss: 2.802612\n",
      "  Step 30/100, Loss: 2.338804\n",
      "  Step 40/100, Loss: 2.870731\n",
      "  Step 50/100, Loss: 2.340734\n",
      "  Step 60/100, Loss: 2.908622\n",
      "  Step 70/100, Loss: 3.335486\n",
      "  Step 80/100, Loss: 2.726668\n",
      "  Step 90/100, Loss: 2.822362\n",
      "  Step 100/100, Loss: 2.523454\n",
      "  Average Loss: 2.717762\n",
      "  Eval MSE: 2.036981, MAE: 1.098488\n",
      "Epoch 5/10\n",
      "  Step 10/100, Loss: 2.600629\n",
      "  Step 20/100, Loss: 2.684796\n",
      "  Step 30/100, Loss: 2.665747\n",
      "  Step 40/100, Loss: 2.688210\n",
      "  Step 50/100, Loss: 3.015015\n",
      "  Step 60/100, Loss: 2.294097\n",
      "  Step 70/100, Loss: 2.858531\n",
      "  Step 80/100, Loss: 2.090029\n",
      "  Step 90/100, Loss: 2.278253\n",
      "  Step 100/100, Loss: 2.519409\n",
      "  Average Loss: 2.569472\n",
      "  Eval MSE: 1.914862, MAE: 1.314428\n",
      "Epoch 6/10\n",
      "  Step 10/100, Loss: 2.992167\n",
      "  Step 20/100, Loss: 2.469287\n",
      "  Step 30/100, Loss: 2.555845\n",
      "  Step 40/100, Loss: 2.585287\n",
      "  Step 50/100, Loss: 2.647686\n",
      "  Step 60/100, Loss: 2.645905\n",
      "  Step 70/100, Loss: 2.672459\n",
      "  Step 80/100, Loss: 3.021347\n",
      "  Step 90/100, Loss: 2.110594\n",
      "  Step 100/100, Loss: 3.062438\n",
      "  Average Loss: 2.676301\n",
      "  Eval MSE: 2.286860, MAE: 1.478158\n",
      "Epoch 7/10\n",
      "  Step 10/100, Loss: 2.757488\n",
      "  Step 20/100, Loss: 2.774537\n",
      "  Step 30/100, Loss: 2.585190\n",
      "  Step 40/100, Loss: 3.183315\n",
      "  Step 50/100, Loss: 2.433384\n",
      "  Step 60/100, Loss: 2.528626\n",
      "  Step 70/100, Loss: 2.709513\n",
      "  Step 80/100, Loss: 2.753980\n",
      "  Step 90/100, Loss: 2.488601\n",
      "  Step 100/100, Loss: 2.179779\n",
      "  Average Loss: 2.639441\n",
      "  Eval MSE: 4.557919, MAE: 2.045892\n",
      "Epoch 8/10\n",
      "  Step 10/100, Loss: 2.623668\n",
      "  Step 20/100, Loss: 2.626043\n",
      "  Step 30/100, Loss: 2.684064\n",
      "  Step 40/100, Loss: 2.649396\n",
      "  Step 50/100, Loss: 2.997595\n",
      "  Step 60/100, Loss: 2.676513\n",
      "  Step 70/100, Loss: 2.534361\n",
      "  Step 80/100, Loss: 2.903832\n",
      "  Step 90/100, Loss: 2.753740\n",
      "  Step 100/100, Loss: 3.078349\n",
      "  Average Loss: 2.752756\n",
      "  Eval MSE: 4.411508, MAE: 1.987133\n",
      "Epoch 9/10\n",
      "  Step 10/100, Loss: 2.578388\n",
      "  Step 20/100, Loss: 3.014438\n",
      "  Step 30/100, Loss: 2.647324\n",
      "  Step 40/100, Loss: 2.898464\n",
      "  Step 50/100, Loss: 2.797271\n",
      "  Step 60/100, Loss: 2.331944\n",
      "  Step 70/100, Loss: 2.667407\n",
      "  Step 80/100, Loss: 2.498975\n",
      "  Step 90/100, Loss: 2.122653\n",
      "  Step 100/100, Loss: 2.199907\n",
      "  Average Loss: 2.575677\n",
      "  Eval MSE: 2.288687, MAE: 1.161899\n",
      "Epoch 10/10\n",
      "  Step 10/100, Loss: 2.216588\n",
      "  Step 20/100, Loss: 2.667307\n",
      "  Step 30/100, Loss: 2.842877\n",
      "  Step 40/100, Loss: 2.658294\n",
      "  Step 50/100, Loss: 2.573234\n",
      "  Step 60/100, Loss: 2.894048\n",
      "  Step 70/100, Loss: 2.902882\n",
      "  Step 80/100, Loss: 2.671645\n",
      "  Step 90/100, Loss: 2.457001\n",
      "  Step 100/100, Loss: 2.763617\n",
      "  Average Loss: 2.664749\n",
      "  Eval MSE: 3.596191, MAE: 1.672308\n"
     ]
    }
   ],
   "source": [
    "# train!\n",
    "dataset = SudokuDataset(data_path='./data/sudoku.csv', num_samples=100000)\n",
    "trainer = CEPredictorTrainer(\n",
    "    predictor=ce_predictor,\n",
    "    dataset=dataset,\n",
    "    mask_schedule=MaskSchedule(schedule_type='linear', total_steps=40),\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.01,\n",
    "    batch_size=32,\n",
    "    ce_batch_size=256,\n",
    "    k=9\n",
    ")\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    avg_loss = trainer.train_epoch(steps_per_epoch=100)\n",
    "    print(f\"  Average Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    eval_metrics = trainer.evaluate(num_samples=100)\n",
    "    print(f\"  Eval MSE: {eval_metrics['mse']:.6f}, MAE: {eval_metrics['mae']:.6f}\")\n",
    "\n",
    "# Save trained predictor\n",
    "torch.save(\n",
    "    ce_predictor.state_dict(),\n",
    "    './checkpoints/ce_predictor_sudoku_mdm.pt'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2116620f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Sudoku from CSV: ./data/sudoku.csv (start=0, n=100000)\n",
      "✓ Loaded 100000 puzzles from CSV: ./data/sudoku.csv\n"
     ]
    }
   ],
   "source": [
    "dataset = SudokuDataset(data_path='./data/sudoku.csv', num_samples=100000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
